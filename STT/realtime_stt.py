import time
import threading
import numpy as np
import sounddevice as sd
from funasr_onnx import SenseVoiceSmall
from funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess
import torch
from collections import deque

class RealTimeSTT:
    """
    VAD(ìŒì„± ê°ì§€), ë§ˆì´í¬ë³„ ì¦í­ ê¸°ëŠ¥ì„ í¬í•¨í•œ ì‹¤ì‹œê°„ STT í´ë˜ìŠ¤.
    (ìƒíƒœ ê¸°ë°˜ VAD ë¡œì§ìœ¼ë¡œ ê°œì„ ëœ ë²„ì „)
    """
    def __init__(self,
                 device_config: dict,
                 on_text_transcribed: callable,
                 model_size: str = "iic/SenseVoiceSmall",
                 language: str = "auto",
                 silence_duration_s: float = 0.5, # ì¹¨ë¬µ ì‹œê°„ì„ ì•½ê°„ ì¤„ì—¬ ë°˜ì‘ì„± ê°œì„ 
                 max_buffer_seconds: int = 30,
                 vad_threshold: float = 0.01): # RMS ì„ê³„ê°’

        self.device_config = device_config
        self.on_text_transcribed = on_text_transcribed
        self.model_size = model_size
        self.language = language
        self.silence_duration_s = silence_duration_s
        self.max_buffer_seconds = max_buffer_seconds
        self.vad_threshold = vad_threshold # RMS ì„ê³„ê°’ìœ¼ë¡œ ì‚¬ìš©

        self.samplerate = 16000
        self.chunk_samples = 1024 # ì•½ 64ms
        self.running = False
        self.threads = []
        self.model = None

    @staticmethod
    def get_available_devices():
        """Get available input devices that are actually usable."""
        devices = sd.query_devices()
        input_devices = []
        
        for i, device in enumerate(devices):
            # Check if device has input channels and is not a disabled/virtual device
            if (device['max_input_channels'] > 0 and 
                device['hostapi'] >= 0 and  # Valid host API
                'Microsoft Sound Mapper' not in device['name'] and  # Skip generic mappers
                'Primary Sound' not in device['name'] and  # Skip primary sound devices
                device['name'].strip()):  # Skip devices with empty names
                
                try:
                    # Test if device is actually accessible
                    with sd.InputStream(device=i, channels=1, samplerate=16000, blocksize=1024):
                        pass
                    input_devices.append({
                        'id': i, 
                        'name': device['name'].strip(),
                        'hostapi_name': sd.query_hostapis(device['hostapi'])['name'],
                        'max_input_channels': device['max_input_channels'],
                        'default_samplerate': device['default_samplerate']
                    })
                except Exception as e:
                    print(f"[STT] Device {i} ({device['name']}) is not accessible: {e}")
                    continue
        
        return input_devices

    def _load_model(self):
        print(f"[STT] '{self.model_size}' ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        quantize = True if device == "cpu" else False
        self.model = SenseVoiceSmall(self.model_size, quantize=quantize, device=device)
        print(f"[STT] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. (ì¥ì¹˜: {device}, ì–‘ìí™”: {quantize})")

    def _process_mic_input(self, device_index: int, config: dict):
        """
        ê°œì„ ëœ ìƒíƒœ ê¸°ë°˜ VAD ë¡œì§ì„ ì‚¬ìš©í•˜ëŠ” ìŠ¤ë ˆë“œ ì›Œì»¤ í•¨ìˆ˜.
        """
        nickname = config["nickname"]
        amplification = config.get("amplification", 1.0)
        
        # --- ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜ ---
        speaking = False
        audio_buffer = deque()
        # ë§ì´ ì‹œì‘ë˜ê¸° ì „ì˜ ì˜¤ë””ì˜¤ë¥¼ ì €ì¥í•˜ì—¬ ë¬¸ë§¥ì„ í™•ë³´ (0.5ì´ˆ)
        pre_buffer = deque(maxlen=int(0.5 * self.samplerate / self.chunk_samples))
        silence_chunks_counter = 0
        max_silence_chunks = int(self.silence_duration_s * self.samplerate / self.chunk_samples)
        max_buffer_chunks = int(self.max_buffer_seconds * self.samplerate / self.chunk_samples)

        try:
            with sd.InputStream(samplerate=self.samplerate,
                                device=device_index,
                                channels=1,
                                blocksize=self.chunk_samples,
                                dtype='float32') as stream:
                print(f"âœ… [{nickname}] ë§ˆì´í¬(ì¥ì¹˜ #{device_index}, ì¦í­: {amplification}x) ì²­ì·¨ ì‹œì‘...")
                
                while self.running:
                    chunk, overflowed = stream.read(self.chunk_samples)
                    if overflowed:
                        print(f"âš ï¸ [{nickname}] ì˜¤ë””ì˜¤ ë²„í¼ ì˜¤ë²„í”Œë¡œìš° ë°œìƒ!")
                    
                    chunk *= amplification
                    is_speech = np.sqrt(np.mean(chunk**2)) > self.vad_threshold

                    # --- ìƒíƒœ ê¸°ë°˜ ë¡œì§ ---
                    if speaking:
                        # 2. ë§í•˜ëŠ” ì¤‘ ìƒíƒœ
                        audio_buffer.append(chunk)
                        
                        if not is_speech:
                            silence_chunks_counter += 1
                        else:
                            silence_chunks_counter = 0 # ë§ì´ ë‹¤ì‹œ ê°ì§€ë˜ë©´ ì¹¨ë¬µ ì¹´ìš´í„° ì´ˆê¸°í™”
                        
                        # ë§ì´ ëë‚¬ë‹¤ê³  íŒë‹¨ (ì¶©ë¶„í•œ ì¹¨ë¬µì´ ì§€ì†) ë˜ëŠ” ë²„í¼ê°€ ê½‰ ì°¼ì„ ë•Œ
                        if silence_chunks_counter > max_silence_chunks or len(audio_buffer) > max_buffer_chunks:
                            speaking = False # ìƒíƒœë¥¼ 'ëŒ€ê¸° ì¤‘'ìœ¼ë¡œ ë³€ê²½
                            
                            # STT ì²˜ë¦¬
                            full_audio = np.concatenate(list(audio_buffer)).flatten()
                            audio_buffer.clear()
                            pre_buffer.clear() # ë²„í¼ ì´ˆê¸°í™”
                            
                            print(f"[STT] ë³€í™˜ ì‹œì‘ (ì˜¤ë””ì˜¤ ê¸¸ì´: {len(full_audio)/self.samplerate:.2f}ì´ˆ)...")
                            res = self.model(full_audio, language=self.language, use_itn=True)
                            transcribed_text = rich_transcription_postprocess(res[0]).strip() if res and res[0] else ""

                            if transcribed_text:
                                try:
                                    self.on_text_transcribed(nickname, transcribed_text)
                                except Exception as e:
                                    print(f"[STT] Error in callback: {e}")
                                    import traceback
                                    traceback.print_exc()
                            else:
                                print("[STT] ë³€í™˜ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        # 1. ëŒ€ê¸° ìƒíƒœ
                        if is_speech:
                            # ë§ì´ ì‹œì‘ë¨ -> 'ë§í•˜ëŠ” ì¤‘' ìƒíƒœë¡œ ë³€ê²½
                            speaking = True
                            print(f"[{nickname}] ìŒì„± ê°ì§€ë¨, ë…¹ìŒ ì‹œì‘...")
                            # pre-bufferì— ìˆë˜ ì˜¤ë””ì˜¤ì™€ í˜„ì¬ ì²­í¬ë¥¼ ë©”ì¸ ë²„í¼ì— ì¶”ê°€
                            audio_buffer.extend(pre_buffer)
                            audio_buffer.append(chunk)
                            silence_chunks_counter = 0
                        else:
                            # ë§ì´ ì—†ì„ ë•ŒëŠ” pre-bufferì—ë§Œ ì˜¤ë””ì˜¤ë¥¼ ì €ì¥
                            pre_buffer.append(chunk)

        except Exception as e:
            print(f"âŒ [{nickname}] ìŠ¤ë ˆë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            print(f"ğŸ›‘ [{nickname}] ë§ˆì´í¬(ì¥ì¹˜ #{device_index}) ì²­ì·¨ ì¤‘ì§€.")

    def start(self):
        if self.running:
            print("[STT] ì´ë¯¸ STTê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
        self._load_model()
        self.running = True
        for device_index, config in self.device_config.items():
            thread = threading.Thread(target=self._process_mic_input, args=(device_index, config))
            self.threads.append(thread)
            thread.start()
        print(f"[STT] ì´ {len(self.threads)}ê°œì˜ ë§ˆì´í¬ì— ëŒ€í•œ STT ì²˜ë¦¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.")

    def stop(self):
        if not self.running:
            return
        print("[STT] STT ì²˜ë¦¬ë¥¼ ì¤‘ì§€í•˜ëŠ” ì¤‘...")
        self.running = False
        for thread in self.threads:
            thread.join()
        self.threads = []
        self.model = None
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("[STT] ëª¨ë“  STT ì²˜ë¦¬ê°€ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- ì‚¬ìš© ì˜ˆì‹œ ---
if __name__ == '__main__':
    def handle_transcription(nickname, text):
        print(f"\n>> [{nickname}] ìµœì¢… ê²°ê³¼: {text}\n")

    print("ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì¥ì¹˜:")
    available_devices = RealTimeSTT.get_available_devices()
    for device in available_devices:
        print(f"  - ID {device['id']}: {device['name']} ({device['hostapi_name']})")
    
    if available_devices:
        mic_id = available_devices[0]['id']
        mic_name = available_devices[0]['name']

        MY_DEVICE_CONFIG = { mic_id: {"nickname": mic_name, "amplification": 1.5} }

        stt_system = RealTimeSTT(
            device_config=MY_DEVICE_CONFIG,
            on_text_transcribed=handle_transcription,
            model_size="iic/SenseVoiceSmall",
            language="ko",
            silence_duration_s=1.5, # 1.5ì´ˆ ì¹¨ë¬µ ì‹œ ì²˜ë¦¬
            vad_threshold=0.01      # ë§ˆì´í¬ í™˜ê²½ì— ë”°ë¼ 0.005 ~ 0.02 ì‚¬ì´ë¡œ ì¡°ì ˆí•´ë³´ì„¸ìš”.
        )

        stt_system.start()
        try:
            print("\nSTT ì‹œìŠ¤í…œì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nì¢…ë£Œ ì‹ í˜¸ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤...")
        finally:
            stt_system.stop()
    else:
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì…ë ¥ ì˜¤ë””ì˜¤ ì¥ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")