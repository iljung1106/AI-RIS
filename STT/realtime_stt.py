import time
import threading
import numpy as np
import sounddevice as sd
from faster_whisper import WhisperModel
import torch
from collections import deque

class RealTimeSTT:
    """
    VAD(ìŒì„± ê°ì§€), ë§ˆì´í¬ë³„ ì¦í­ ê¸°ëŠ¥ì„ í¬í•¨í•œ ì‹¤ì‹œê°„ STT í´ë˜ìŠ¤.
    ì‚¬ìš©ìê°€ ë§ì„ ë©ˆì¶”ë©´ ë…¹ìŒëœ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        device_config (dict): ë§ˆì´í¬ ì¥ì¹˜ ì„¤ì •.
            - í˜•ì‹: {device_index: {"nickname": str, "amplification": float}}
            - ì˜ˆ: {0: {"nickname": "ë§ˆì´í¬1", "amplification": 1.5}}
        on_text_transcribed (callable): í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œ í˜¸ì¶œë  ì½œë°± í•¨ìˆ˜.
            - ì¸ì: (nickname, text)
        model_size (str): Whisper ëª¨ë¸ í¬ê¸° ë˜ëŠ” HuggingFace ì €ì¥ì†Œ ID.
        language (str): ì¸ì‹í•  ì–¸ì–´ ì½”ë“œ (e.g., "ko", "en").
        silence_duration_s (float): ìŒì„±ìœ¼ë¡œ ê°„ì£¼í•˜ì§€ ì•Šì„ ì¹¨ë¬µì˜ ì‹œê°„ (ì´ˆ).
        max_buffer_seconds (int): ìµœëŒ€ ë…¹ìŒ ë²„í¼ í¬ê¸° (ì´ˆ). ì´ ì‹œê°„ì„ ì´ˆê³¼í•˜ë©´ ìë™ìœ¼ë¡œ ì²˜ë¦¬.
        vad_threshold (float): VADì˜ ìŒì„± ê°ì§€ ë¯¼ê°ë„ (0.0 ~ 1.0). ë‚®ì„ìˆ˜ë¡ ë¯¼ê°.
    """
    def __init__(self,
                 device_config: dict,
                 on_text_transcribed: callable,
                 model_size: str = "deepdml/faster-whisper-large-v3-turbo-ct2",
                 language: str = "ko",
                 silence_duration_s: float = 2.0,
                 max_buffer_seconds: int = 30,
                 vad_threshold: float = 0.5):

        self.device_config = device_config
        self.on_text_transcribed = on_text_transcribed
        self.model_size = model_size
        self.language = language
        self.silence_duration_s = silence_duration_s
        self.max_buffer_seconds = max_buffer_seconds
        self.vad_threshold = vad_threshold

        self.samplerate = 16000  # Whisper ëª¨ë¸ì˜ ìš”êµ¬ì‚¬í•­
        self.chunk_samples = 1024  # ì‘ì€ ì˜¤ë””ì˜¤ ì¡°ê° í¬ê¸°
        self.running = False
        self.threads = []
        self.model = None

    @staticmethod
    def get_available_devices():
        """Returns a list of available audio input devices."""
        devices = sd.query_devices()
        input_devices = []
        for i, device in enumerate(devices):
            # Check if it's an input device (has input channels)
            if device['max_input_channels'] > 0:
                input_devices.append({'id': i, 'name': device['name'], 'hostapi_name': device['hostapi']})
        return input_devices

    def _load_model(self):
        """Whisper ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        print(f"[STT] '{self.model_size}' ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        compute_type = "float16" if device == "cuda" else "int8"
        
        if device == "cpu" and compute_type == "float16":
            print("[STT] ê²½ê³ : CPUì—ì„œëŠ” float16ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. compute_typeì„ 'int8'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
            compute_type = "int8"
            
        self.model = WhisperModel(self.model_size, device=device, compute_type=compute_type)
        print(f"[STT] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. (ì¥ì¹˜: {device}, ê³„ì‚° íƒ€ì…: {compute_type})")

    def _process_mic_input(self, device_index: int, config: dict):
        """
        ê°œë³„ ë§ˆì´í¬ ì…ë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ìŠ¤ë ˆë“œ ì›Œì»¤ í•¨ìˆ˜.
        """
        nickname = config["nickname"]
        amplification = config.get("amplification", 1.0)
        
        audio_buffer = deque()
        silence_chunks = 0
        max_silence_chunks = int(self.silence_duration_s * self.samplerate / self.chunk_samples)
        max_buffer_chunks = int(self.max_buffer_seconds * self.samplerate / self.chunk_samples)

        try:
            with sd.InputStream(samplerate=self.samplerate,
                                device=device_index,
                                channels=1,
                                blocksize=self.chunk_samples,
                                dtype='float32') as stream:
                print(f"âœ… [{nickname}] ë§ˆì´í¬(ì¥ì¹˜ #{device_index}, ì¦í­: {amplification}x) ì²­ì·¨ ì‹œì‘...")
                
                while self.running:
                    chunk, overflowed = stream.read(self.chunk_samples)
                    if overflowed:
                        print(f"âš ï¸ [{nickname}] ì˜¤ë””ì˜¤ ë²„í¼ ì˜¤ë²„í”Œë¡œìš° ë°œìƒ!")
                    
                    # 1. ì†Œë¦¬ ì¦í­
                    chunk *= amplification
                    
                    # 2. ìŒì„± í™œë™ ê°ì§€ (ê°„ë‹¨í•œ RMS ê¸°ë°˜)
                    # VAD í•„í„°ëŠ” ê¸´ ì˜¤ë””ì˜¤ì— ë” íš¨ê³¼ì ì´ë¯€ë¡œ, ì‹¤ì‹œê°„ ì²­í¬ëŠ” RMSë¡œ íŒë‹¨
                    is_speech = np.sqrt(np.mean(chunk**2)) > 0.01  # ì„ê³„ê°’, í™˜ê²½ì— ë”°ë¼ ì¡°ì ˆ
                    
                    if is_speech:
                        silence_chunks = 0
                        audio_buffer.append(chunk)
                        # print(f"[STT] Speech detected. Buffer size: {len(audio_buffer)} ") # Too noisy
                    else:
                        silence_chunks += 1
                    
                    # 3. STT ì²˜ë¦¬ ì¡°ê±´ í™•ì¸
                    # ì¡°ê±´ A: ë§ì´ ëë‚˜ê³  ì¼ì • ì‹œê°„ ì¹¨ë¬µì´ íë¥¸ ê²½ìš°
                    # ì¡°ê±´ B: ë²„í¼ê°€ ë„ˆë¬´ ê¸¸ì–´ì§„ ê²½ìš° (ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥)
                    should_transcribe = (len(audio_buffer) > 0 and silence_chunks > max_silence_chunks) or \
                                        (len(audio_buffer) > max_buffer_chunks)

                    if should_transcribe:
                        print(f"[STT] Transcribing audio buffer (size: {len(audio_buffer)} chunks).")
                        # ë²„í¼ì˜ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨
                        full_audio = np.concatenate(list(audio_buffer)).flatten()
                        audio_buffer.clear()
                        silence_chunks = 0
                        
                        # faster-whisper ëª¨ë¸ë¡œ STT ìˆ˜í–‰
                        segments, _ = self.model.transcribe(
                            full_audio,
                            language=self.language,
                            vad_filter=True,
                            vad_parameters={"threshold": self.vad_threshold}
                        )
                        
                        transcribed_text = "".join(segment.text for segment in segments).strip()

                        if transcribed_text:
                            print(f"[STT] Transcribed text: '{transcribed_text}'. Calling callback.")
                            self.on_text_transcribed(nickname, transcribed_text)
                        else:
                            print("[STT] Transcribed text is empty or only whitespace.")

        except Exception as e:
            print(f"âŒ [{nickname}] ìŠ¤ë ˆë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            print(f"ğŸ›‘ [{nickname}] ë§ˆì´í¬(ì¥ì¹˜ #{device_index}) ì²­ì·¨ ì¤‘ì§€.")


    def start(self):
        """STT ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.running:
            print("[STT] ì´ë¯¸ STTê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return

        self._load_model()
        self.running = True

        for device_index, config in self.device_config.items():
            thread = threading.Thread(target=self._process_mic_input, args=(device_index, config))
            self.threads.append(thread)
            thread.start()
        
        print(f"[STT] ì´ {len(self.threads)}ê°œì˜ ë§ˆì´í¬ì— ëŒ€í•œ STT ì²˜ë¦¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.")


    def stop(self):
        """STT ì²˜ë¦¬ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        if not self.running:
            return

        print("[STT] STT ì²˜ë¦¬ë¥¼ ì¤‘ì§€í•˜ëŠ” ì¤‘...")
        self.running = False
        for thread in self.threads:
            thread.join()
        
        self.threads = []
        self.model = None
        print("[STT] ëª¨ë“  STT ì²˜ë¦¬ê°€ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
